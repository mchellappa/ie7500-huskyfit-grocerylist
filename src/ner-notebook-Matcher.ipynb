{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8068ac8-5091-4b88-a9fe-7b23b3efb33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af77c757-0a09-4e33-8c33-67af496827fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('recipe_dataset_large.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f2f0d-1059-4b6e-a62e-c6ccbd3af4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00300cfa-1801-4294-b1bc-9d38c2287ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd1f37-0b1f-4b6b-89a9-cb2acdb4f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['combined'] = data['title'] + ' ' + data['ingredients'] + ' ' + data['directions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99278a92-9a0a-416f-9813-5a74c7c34d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['combined'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cdbcbc-b83d-4445-96a0-96f7c83327f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['combined'].tolist()\n",
    "\n",
    "ner_tags = data['NER'].apply(eval).tolist()  # Convert string representations of lists to actual lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90043c1e-aa01-4a9c-ae62-ca0a1d851d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[89]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6733167a-d9cb-4372-85da-d3ec82e3c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tags[89]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad91c2-89e5-4ae0-95b3-d98613733f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of terms\n",
    "terms = {}\n",
    "patterns = []\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "for tags in ner_tags:\n",
    "    for tag in tags:\n",
    "        if tag.lower() not in terms:\n",
    "            terms[tag.lower()] = {'label': 'INGREDIENT'}\n",
    "            patterns.append(nlp(tag.lower()))\n",
    "\n",
    "# Initialize the PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)  # nlp.vocab ~ A storage class for vocabulary and other data shared across a language\n",
    "matcher.add(\"INGREDIENT\", None, *patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3586df0c-f903-46e6-9bc0-e652e2427917",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60586022",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"ingredient_extractor\")\n",
    "def ingredient_extractor(doc):\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end, label='INGREDIENT') for match_id, start, end in matches]\n",
    "\n",
    "    # Resolve overlaps by keeping the longest span\n",
    "    filtered_spans = spacy.util.filter_spans(spans)\n",
    "    \n",
    "    doc.ents = filtered_spans\n",
    "    return doc\n",
    "# Add the custom component to the pipeline\n",
    "nlp.add_pipe(\"ingredient_extractor\", last=True)\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "@Language.component(\"quantity_extractor\")\n",
    "def quantity_extractor(doc):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [\n",
    "        {\"LIKE_NUM\": True},  # Match numbers\n",
    "        {\"LOWER\": {\"IN\": [\"cup\", \"cups\", \"tablespoon\", \"tablespoons\", \"tsp\", \"teaspoon\", \"teaspoons\", \"oz\", \"ounce\", \"ounces\", \"pound\", \"pounds\", \"lb\", \"lbs\", \"gram\", \"grams\", \"kg\", \"kilogram\", \"kilograms\"]}}\n",
    "    ]\n",
    "    matcher.add(\"QUANTITY\", [pattern])\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end, label=\"QUANTITY\") for match_id, start, end in matches]\n",
    "    filtered_spans = spacy.util.filter_spans(spans)\n",
    "    \n",
    "    # Ensure no overlapping entities\n",
    "    new_ents = [ent for ent in doc.ents if ent.label_ != \"QUANTITY\"]\n",
    "    doc.ents = new_ents + filtered_spans\n",
    "    return doc\n",
    "nlp.add_pipe(\"quantity_extractor\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80924bca-62b8-44ae-acaf-f023d9337745",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2e3cf-bb0c-4350-9826-6f805ec09317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "train_data = [(text, {\"entities\": []}) for text in texts]\n",
    "\n",
    "for i, (text, annotations) in enumerate(train_data):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "    train_data[i] = (text, {\"entities\": entities})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de96b18e-1e5b-48cb-adcc-2455e851cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_data(data, output_file):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    doc_bin = DocBin()\n",
    "    for text, annotations in data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in annotations[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span is not None:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        doc_bin.add(doc)\n",
    "    doc_bin.to_disk(output_file)\n",
    "\n",
    "save_training_data(train_data, 'training_data.spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbeb811-fd6e-4eeb-8763-afb02bcd1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# Load the training data\n",
    "nlp = spacy.blank(\"en\")\n",
    "db = DocBin().from_disk(\"training_data.spacy\")\n",
    "docs = list(db.get_docs(nlp.vocab))\n",
    "\n",
    "# Create the NER component and add it to the pipeline\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\", last=True)\n",
    "\n",
    "# Add the labels to the NER component\n",
    "for _, annotations in train_data:\n",
    "    for ent in annotations.get('entities'):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "# Disable other pipes during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(5):  # 5 iterations\n",
    "        random.shuffle(docs)\n",
    "        losses = {}\n",
    "        batches = minibatch(docs, size=compounding(4.0, 32.0, 1.5))\n",
    "        for batch in batches:\n",
    "            for doc in batch:\n",
    "                example = Example.from_dict(doc, {\"entities\": [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]})\n",
    "                nlp.update([example], drop=0.5, losses=losses)\n",
    "        print(f\"Iteration {itn}, Losses: {losses}\")\n",
    "\n",
    "# Save the trained model to disk\n",
    "nlp.to_disk(\"ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf65ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the custom NER\n",
    "doc = nlp(\"Add 2 cups of flour and 1 tablespoon of sugar.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526825b-2ca9-4c67-b1cc-7cad473ae465",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd5f563-8f77-4159-b853-f004870d8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data):\n",
    "    examples = [Example.from_dict(nlp.make_doc(text), annotations) for text, annotations in test_data]\n",
    "    scorer = model.evaluate(examples)\n",
    "    return scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea41228a-1128-439d-b2ae-ce5a09334aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = evaluate_model(nlp, train_data)\n",
    "print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c910cc-502d-4158-b96d-4fc0060ae61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on some examples\n",
    "test_texts = \"In a heavy 2-quart saucepan, mix 2 lbs brown sugar, nuts, evaporated milk and butter or margarine. Stir over medium heat until mixture bubbles all over top. Boil and stir 5 minutes more. Take off heat. Stir in vanilla and cereal; mix well. Using 2 teaspoons, drop and shape into 30 clusters on wax paper.Let stand until firm, about 30 minutes.\"\n",
    "doc = nlp(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffba41b-17b7-496c-bfed-49a85a72f43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73fc503-abb6-4b9b-b811-739f9b33c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_text = \"\"\"Peel potatoes and keep aside\n",
    "Heat oil in a wok\n",
    "Crackle mustard seeds and urad dal\n",
    "Add Kari Patta\n",
    "soute ginger garlic paste and chopped onion\n",
    "Aad potatoes and soute for 1 to 2minutes\n",
    "Add all dry masala\n",
    "Cook with lid on slow flame for 10 minutes\n",
    "Garnish with chopped coriander\n",
    "Serve hot with roti or poori\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b181f16-85a3-4051-a6a0-a582fcc04d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(real_text)\n",
    "\n",
    "displacy.render(doc2, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63011bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "entities = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"QUANTITY\":\n",
    "        for i_ent in doc.ents:\n",
    "            if i_ent.label_ == \"INGREDIENT\" and i_ent.start == ent.end:\n",
    "                entities.append({\"ingredient\": i_ent.text, \"quantity\": ent.text})\n",
    "                break\n",
    "    elif ent.label_ == \"INGREDIENT\":\n",
    "        if not any(e[\"ingredient\"] == ent.text for e in entities):\n",
    "            entities.append({\"ingredient\": ent.text, \"quantity\": \"\"})\n",
    "\n",
    "entities_json = json.dumps(entities, indent=2)\n",
    "\n",
    "print(entities_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
