{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8068ac8-5091-4b88-a9fe-7b23b3efb33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af77c757-0a09-4e33-8c33-67af496827fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('recipe_dataset_large.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f2f0d-1059-4b6e-a62e-c6ccbd3af4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00300cfa-1801-4294-b1bc-9d38c2287ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffd1f37-0b1f-4b6b-89a9-cb2acdb4f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['combined'] = data['title'] + ' ' + data['ingredients'] + ' ' + data['directions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99278a92-9a0a-416f-9813-5a74c7c34d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['combined'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cdbcbc-b83d-4445-96a0-96f7c83327f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data['combined'].tolist()\n",
    "\n",
    "ner_tags = data['NER'].apply(eval).tolist()  # Convert string representations of lists to actual lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90043c1e-aa01-4a9c-ae62-ca0a1d851d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[89]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6733167a-d9cb-4372-85da-d3ec82e3c68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tags[89]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad91c2-89e5-4ae0-95b3-d98613733f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of terms\n",
    "terms = {}\n",
    "patterns = []\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "for tags in ner_tags:\n",
    "    for tag in tags:\n",
    "        if tag.lower() not in terms:\n",
    "            terms[tag.lower()] = {'label': 'INGREDIENT'}\n",
    "            patterns.append(nlp(tag.lower()))\n",
    "\n",
    "# Initialize the PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)  # nlp.vocab ~ A storage class for vocabulary and other data shared across a language\n",
    "matcher.add(\"INGREDIENT\", None, *patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3586df0c-f903-46e6-9bc0-e652e2427917",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60586022",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"ingredient_extractor\")\n",
    "def ingredient_extractor(doc):\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end, label='INGREDIENT') for match_id, start, end in matches]\n",
    "\n",
    "    # Resolve overlaps by keeping the longest span\n",
    "    filtered_spans = spacy.util.filter_spans(spans)\n",
    "    \n",
    "    doc.ents = filtered_spans\n",
    "    return doc\n",
    "# Add the custom component to the pipeline\n",
    "nlp.add_pipe(\"ingredient_extractor\", last=True)\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "@Language.component(\"quantity_extractor\")\n",
    "def quantity_extractor(doc):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [\n",
    "        {\"LIKE_NUM\": True},  # Match numbers\n",
    "        {\"LOWER\": {\"IN\": [\"cup\", \"cups\", \"tablespoon\", \"tablespoons\", \"tsp\", \"teaspoon\", \"teaspoons\", \"oz\", \"ounce\", \"ounces\", \"pound\", \"pounds\", \"lb\", \"lbs\", \"gram\", \"grams\", \"kg\", \"kilogram\", \"kilograms\"]}}\n",
    "    ]\n",
    "    matcher.add(\"QUANTITY\", [pattern])\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end, label=\"QUANTITY\") for match_id, start, end in matches]\n",
    "    filtered_spans = spacy.util.filter_spans(spans)\n",
    "    \n",
    "    # Ensure no overlapping entities\n",
    "    new_ents = [ent for ent in doc.ents if ent.label_ != \"QUANTITY\"]\n",
    "    doc.ents = new_ents + filtered_spans\n",
    "    return doc\n",
    "nlp.add_pipe(\"quantity_extractor\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80924bca-62b8-44ae-acaf-f023d9337745",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2e3cf-bb0c-4350-9826-6f805ec09317",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "train_data = [(text, {\"entities\": []}) for text in texts]\n",
    "\n",
    "for i, (text, annotations) in enumerate(train_data):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "    train_data[i] = (text, {\"entities\": entities})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de96b18e-1e5b-48cb-adcc-2455e851cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_data(data, output_file):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    doc_bin = DocBin()\n",
    "    for text, annotations in data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in annotations[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span is not None:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        doc_bin.add(doc)\n",
    "    doc_bin.to_disk(output_file)\n",
    "\n",
    "save_training_data(train_data, 'training_data.spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbeb811-fd6e-4eeb-8763-afb02bcd1e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# Load the training data\n",
    "nlp = spacy.blank(\"en\")\n",
    "db = DocBin().from_disk(\"training_data.spacy\")\n",
    "docs = list(db.get_docs(nlp.vocab))\n",
    "\n",
    "# Create the NER component and add it to the pipeline\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\", last=True)\n",
    "\n",
    "# Add the labels to the NER component\n",
    "for _, annotations in train_data:\n",
    "    for ent in annotations.get('entities'):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "# Disable other pipes during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(5):  # 5 iterations\n",
    "        random.shuffle(docs)\n",
    "        losses = {}\n",
    "        batches = minibatch(docs, size=compounding(4.0, 32.0, 1.5))\n",
    "        for batch in batches:\n",
    "            for doc in batch:\n",
    "                example = Example.from_dict(doc, {\"entities\": [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]})\n",
    "                nlp.update([example], drop=0.5, losses=losses)\n",
    "        print(f\"Iteration {itn}, Losses: {losses}\")\n",
    "\n",
    "# Save the trained model to disk\n",
    "nlp.to_disk(\"ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf65ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the custom NER\n",
    "doc = nlp(\"Add 2 cups of flour and 1 tablespoon of sugar.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c526825b-2ca9-4c67-b1cc-7cad473ae465",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd5f563-8f77-4159-b853-f004870d8e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data):\n",
    "    examples = [Example.from_dict(nlp.make_doc(text), annotations) for text, annotations in test_data]\n",
    "    scorer = model.evaluate(examples)\n",
    "    return scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea41228a-1128-439d-b2ae-ce5a09334aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = evaluate_model(nlp, train_data)\n",
    "print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c910cc-502d-4158-b96d-4fc0060ae61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on some examples\n",
    "test_texts = \"In a heavy 2-quart saucepan, mix 2 lbs brown sugar, nuts, evaporated milk and butter or margarine. Stir over medium heat until mixture bubbles all over top. Boil and stir 5 minutes more. Take off heat. Stir in vanilla and cereal; mix well. Using 2 teaspoons, drop and shape into 30 clusters on wax paper.Let stand until firm, about 30 minutes.\"\n",
    "doc = nlp(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffba41b-17b7-496c-bfed-49a85a72f43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73fc503-abb6-4b9b-b811-739f9b33c94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_text = \"\"\"Peel potatoes and keep aside\n",
    "Heat oil in a wok\n",
    "Crackle mustard seeds and urad dal\n",
    "Add Kari Patta\n",
    "soute ginger garlic paste and chopped onion\n",
    "Aad potatoes and soute for 1 to 2minutes\n",
    "Add all dry masala\n",
    "Cook with lid on slow flame for 10 minutes\n",
    "Garnish with chopped coriander\n",
    "Serve hot with roti or poori\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b181f16-85a3-4051-a6a0-a582fcc04d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(real_text)\n",
    "\n",
    "displacy.render(doc2, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63011bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "entities = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"QUANTITY\":\n",
    "        for i_ent in doc.ents:\n",
    "            if i_ent.label_ == \"INGREDIENT\" and i_ent.start == ent.end:\n",
    "                entities.append({\"ingredient\": i_ent.text, \"quantity\": ent.text})\n",
    "                break\n",
    "    elif ent.label_ == \"INGREDIENT\":\n",
    "        if not any(e[\"ingredient\"] == ent.text for e in entities):\n",
    "            entities.append({\"ingredient\": ent.text, \"quantity\": \"\"})\n",
    "\n",
    "entities_json = json.dumps(entities, indent=2)\n",
    "\n",
    "print(entities_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365641e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training import Example\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def evaluate_model(model, test_data):\n",
    "\texamples = [Example.from_dict(model.make_doc(text), annotations) for text, annotations in test_data]\n",
    "\tscorer = Scorer()\n",
    "\tfor example in examples:\n",
    "\t\tmodel.update([example], sgd=None, losses={})\n",
    "\t\tscorer.score([example])  # Pass a list containing the example\n",
    "\treturn scorer.score(examples)  # Use the score method to get the evaluation results\n",
    "\n",
    "evaluation_results = evaluate_model(nlp, train_data)\n",
    "print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455fe49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"recipe_dataset_large.csv\")  # Replace with your actual file path\n",
    "\n",
    "# Split the dataset into 80% training and 20% testing\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Testing set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc561b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import re\n",
    "import ast\n",
    "\n",
    "def extract_quantities(text):\n",
    "    \"\"\"Extract quantities from the text.\"\"\"\n",
    "    quantity_patterns = [\n",
    "        r'\\b\\d+\\s*(?:cup|cups|tbsp|tablespoon|tablespoons|tsp|teaspoon|teaspoons|oz|ounce|ounces|lb|pound|pounds|g|gram|grams|kg|kilogram|kilograms|ml|milliliter|milliliters|l|liter|liters)\\b',\n",
    "        r'\\b\\d+/\\d+\\s*(?:cup|cups|tbsp|tablespoon|tablespoons|tsp|teaspoon|teaspoons|oz|ounce|ounces|lb|pound|pounds|g|gram|grams|kg|kilogram|kilograms|ml|milliliter|milliliters|l|liter|liters)\\b',\n",
    "        r'\\b\\d+\\s*(?:and\\s*\\d+/\\d+)?\\s*(?:cup|cups|tbsp|tablespoon|tablespoons|tsp|teaspoon|teaspoons|oz|ounce|ounces|lb|pound|pounds|g|gram|grams|kg|kilogram|kilograms|ml|milliliter|milliliters|l|liter|liters)\\b',\n",
    "        r'\\b\\d+\\s*(?:-\\s*\\d+)?\\s*(?:cup|cups|tbsp|tablespoon|tablespoons|tsp|teaspoon|teaspoons|oz|ounce|ounces|lb|pound|pounds|g|gram|grams|kg|kilogram|kilograms|ml|milliliter|milliliters|l|liter|liters)\\b',\n",
    "        r'\\b\\d+\\s*(?:to\\s*\\d+)?\\s*(?:cup|cups|tbsp|tablespoon|tablespoons|tsp|teaspoon|teaspoons|oz|ounce|ounces|lb|pound|pounds|g|gram|grams|kg|kilogram|kilograms|ml|milliliter|milliliters|l|liter|liters)\\b',\n",
    "        r'\\b\\d+\\s*(?:\\.\\d+)?\\s*(?:cup|cups|tbsp|tablespoon|tablespoons|tsp|teaspoon|teaspoons|oz|ounce|ounces|lb|pound|pounds|g|gram|grams|kg|kilogram|kilograms|ml|milliliter|milliliters|l|liter|liters)\\b',\n",
    "        r'\\b½\\s*(?:cup|cups|tbsp|tablespoon|tablespoons|tsp|teaspoon|teaspoons|oz|ounce|ounces|lb|pound|pounds|g|gram|grams|kg|kilogram|kilograms|ml|milliliter|milliliters|l|liter|liters)\\b'\n",
    "    ]\n",
    "    entities = []\n",
    "    for pattern in quantity_patterns:\n",
    "        matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            entities.append((match.start(), match.end(), \"QUANTITY\"))\n",
    "    return entities\n",
    "\n",
    "def get_clean_ingredients(ingredient_list):\n",
    "    \"\"\"Clean the ingredients list to be used for NER.\"\"\"\n",
    "    try:\n",
    "        return ast.literal_eval(ingredient_list)  # Convert string to list\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "def extract_ingredients(text, ingredients):\n",
    "    \"\"\"Extract ingredients from the text.\"\"\"\n",
    "    entities = []\n",
    "    for ingredient in ingredients:\n",
    "        start_idx = text.lower().find(ingredient.lower())\n",
    "        if start_idx != -1:\n",
    "            end_idx = start_idx + len(ingredient)\n",
    "            entities.append((start_idx, end_idx, \"INGREDIENT\"))\n",
    "    return entities\n",
    "\n",
    "# Initialize SpaCy blank English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin()\n",
    "\n",
    "# Process the training data\n",
    "for _, row in train_df.iterrows():\n",
    "    recipe_text = row['ingredients']  # Assuming 'ingredients' column has the list of ingredients\n",
    "    ner_labels = get_clean_ingredients(row['NER'])  # Clean the NER column\n",
    "\n",
    "    # Extract both quantities and ingredients from the text\n",
    "    quantity_entities = extract_quantities(recipe_text)\n",
    "    ingredient_entities = extract_ingredients(recipe_text, ner_labels)\n",
    "\n",
    "    # Combine quantity and ingredient entities\n",
    "    all_entities = quantity_entities + ingredient_entities\n",
    "\n",
    "    # Make sure there are no overlapping entities\n",
    "    added_spans = []\n",
    "    entities_to_add = []\n",
    "    doc = nlp.make_doc(recipe_text)\n",
    "\n",
    "    for start, end, label in all_entities:\n",
    "        # Check for overlapping spans\n",
    "        if all(not (start < existing_end and end > existing_start) for existing_start, existing_end in added_spans):\n",
    "            span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "            if span:\n",
    "                entities_to_add.append(span)\n",
    "                added_spans.append((start, end))\n",
    "\n",
    "    # Assign entities to the doc and add to DocBin\n",
    "    if entities_to_add:\n",
    "        doc.ents = entities_to_add\n",
    "        doc_bin.add(doc)\n",
    "\n",
    "# Save the processed training data in SpaCy format\n",
    "doc_bin.to_disk(\"ner_training_data.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c1bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Example\n",
    "\n",
    "# Load the training data\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin().from_disk(\"ner_training_data.spacy\")\n",
    "docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "# Create a new entity recognizer\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Add labels to the entity recognizer\n",
    "for doc in docs:\n",
    "    for ent in doc.ents:\n",
    "        ner.add_label(ent.label_)\n",
    "\n",
    "# Disable other pipelines during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(20):  # Number of iterations\n",
    "        for doc in docs:\n",
    "            example = Example.from_dict(doc, {\"entities\": [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]})\n",
    "            nlp.update([example], drop=0.5, sgd=optimizer)\n",
    "\n",
    "# Save the model\n",
    "nlp.to_disk(\"./output/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example\n",
    "\n",
    "# Load the trained model\n",
    "nlp = spacy.load(\"./output/model-best\")\n",
    "\n",
    "# Prepare the test data\n",
    "test_docs = []\n",
    "for _, row in test_df.iterrows():\n",
    "    recipe_text = row['ingredients']  # Assuming 'ingredients' column has the list of ingredients\n",
    "    ner_labels = get_clean_ingredients(row['NER'])  # Clean the NER column\n",
    "\n",
    "    # Extract both quantities and ingredients from the text\n",
    "    quantity_entities = extract_quantities(recipe_text)\n",
    "    ingredient_entities = extract_ingredients(recipe_text, ner_labels)\n",
    "\n",
    "    # Combine quantity and ingredient entities\n",
    "    all_entities = quantity_entities + ingredient_entities\n",
    "\n",
    "    # Create a SpaCy Example object\n",
    "    doc = nlp.make_doc(recipe_text)\n",
    "    example = Example.from_dict(doc, {\"entities\": [(start, end, label) for start, end, label in all_entities]})\n",
    "    test_docs.append(example)\n",
    "\n",
    "# Evaluate the model\n",
    "scorer = Scorer()\n",
    "for example in test_docs:\n",
    "    nlp.update([example], drop=0.0)\n",
    "    scorer.score(example)\n",
    "\n",
    "# Print evaluation results\n",
    "print(scorer.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b426459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the custom NER\n",
    "doc = nlp(\"Add 2 cups of flour and 1 tablespoon of sugar.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb406d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
