{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b636db5-c456-4012-aa49-7cc4c49fc5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f367902-55b3-4630-8341-2f26e62b7f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('preprocessed_labelled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6df7a22-d1d7-4260-9dcc-6e7a65b55945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NER</th>\n",
       "      <th>normalized_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['bite size shredded rice biscuits', 'vanilla'...</td>\n",
       "      <td>no-bake nut cookies 1 cup firmly packed brown ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['cream of mushroom soup', 'beef', 'sour cream...</td>\n",
       "      <td>jewell ball s chicken 1 small jar chipped beef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['frozen corn', 'pepper', 'cream cheese', 'gar...</td>\n",
       "      <td>creamy corn 2 16 ounce package frozen corn 1 8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['graham cracker crumbs', 'powdered sugar', 'p...</td>\n",
       "      <td>reeses cup candy 1 cup peanut butter 0.75 cup ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['sour cream', 'bacon', 'pepper', 'extra lean ...</td>\n",
       "      <td>cheeseburger potato soup 6 baking potatoes 1 p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 NER  \\\n",
       "0  ['bite size shredded rice biscuits', 'vanilla'...   \n",
       "1  ['cream of mushroom soup', 'beef', 'sour cream...   \n",
       "2  ['frozen corn', 'pepper', 'cream cheese', 'gar...   \n",
       "3  ['graham cracker crumbs', 'powdered sugar', 'p...   \n",
       "4  ['sour cream', 'bacon', 'pepper', 'extra lean ...   \n",
       "\n",
       "                                 normalized_combined  \n",
       "0  no-bake nut cookies 1 cup firmly packed brown ...  \n",
       "1  jewell ball s chicken 1 small jar chipped beef...  \n",
       "2  creamy corn 2 16 ounce package frozen corn 1 8...  \n",
       "3  reeses cup candy 1 cup peanut butter 0.75 cup ...  \n",
       "4  cheeseburger potato soup 6 baking potatoes 1 p...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8cb3c1a-7a53-41e4-a8da-b4653369b305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1930209"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "917c2d24-1e5b-48b7-bf53-bb79e53e6c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = data['normalized_combined'].tolist()\n",
    "type(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91ec73c4-1fe6-41d1-90c2-769ba2045754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['bite size shredded rice biscuits', 'vanilla', 'brown sugar', 'nuts', 'milk', 'butter']\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tags = data['NER'].tolist()\n",
    "ner_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17edeb4a-0f33-444b-9f98-e60eb4e4e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of terms\n",
    "terms = {}\n",
    "patterns = []\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "for tags in ner_tags:\n",
    "    for tag in tags:\n",
    "        if tag not in terms and tag!='mix':\n",
    "            terms[tag] = {'label': 'INGREDIENT'}\n",
    "            patterns.append(nlp(tag))\n",
    "\n",
    "# Initialize the PhraseMatcher\n",
    "ingredient_matcher = PhraseMatcher(nlp.vocab)  # nlp.vocab ~ A storage class for vocabulary and other data shared across a language\n",
    "ingredient_matcher.add(\"INGREDIENT\", None, *patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33b1d2e2-4b6c-422f-a1f7-99968341ecac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {}, 'problems': {}, 'attrs': {}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a75086ba-f253-454e-a5e3-0ed8878838cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.quantity_extractor(doc)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@Language.component(\"ingredient_extractor\")\n",
    "def ingredient_extractor(doc):\n",
    "    matches = ingredient_matcher(doc)\n",
    "    spans = [Span(doc, start, end, label='INGREDIENT') for match_id, start, end in matches]\n",
    "\n",
    "    # Resolve overlaps by keeping the longest span\n",
    "    filtered_spans = spacy.util.filter_spans(spans)\n",
    "    \n",
    "    doc.ents = filtered_spans\n",
    "    return doc\n",
    "# Add the custom component to the pipeline\n",
    "nlp.add_pipe(\"ingredient_extractor\", last=True)\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "@Language.component(\"quantity_extractor\")\n",
    "def quantity_extractor(doc):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern = [\n",
    "        {\"LIKE_NUM\": True},  # Match numbers\n",
    "        {\"LIKE_NUM\": True, \"OP\": \"?\"},  # Match the second number (e.g., 8), but optional\n",
    "        {\"LOWER\": {\"IN\": [\"cup\", \"tablespoon\", \"teaspoon\", \"ounce\", \"pound\", \"gram\", \"kilogram\",\"package\", \"quart\", \"liter\", \"milliliter\"]}}\n",
    "    ]\n",
    "    matcher.add(\"QUANTITY\", [pattern])\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end, label=\"QUANTITY\") for match_id, start, end in matches]\n",
    "    filtered_spans = spacy.util.filter_spans(spans)\n",
    "    \n",
    "    # Ensure no overlapping entities\n",
    "    new_ents = [ent for ent in doc.ents if ent.label_ != \"QUANTITY\"]\n",
    "    doc.ents = new_ents + filtered_spans\n",
    "    return doc\n",
    "nlp.add_pipe(\"quantity_extractor\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "016ecba2-1ba9-484b-b820-a6f04d0b68cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'ingredient_extractor': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'quantity_extractor': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'ingredient_extractor': [], 'quantity_extractor': []},\n",
       " 'attrs': {}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea475e8-f7e7-44f9-938f-ea449e308fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "\n",
    "train_data = [(text, {\"entities\": []}) for text in texts]\n",
    "\n",
    "for i, (text, annotations) in enumerate(train_data):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "    train_data[i] = (text, {\"entities\": entities})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6b096cb-4fcc-492c-8306-c30e0c7471ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: no-bake nut cookies 1 cup firmly packed brown sugar 0.5 cup evaporated milk 0.5 teaspoon vanilla 0.5 cup broken nuts pecans 2 tablespoon butter or margarine 3.5 cup bite size shredded rice biscuits in a heavy 2 quart saucepan mix brown sugar nuts evaporated milk and butter or margarine stir over medium heat until mixture bubbles all over top boil and stir 5 minutes more take off heat stir in vanilla and cereal mix well using 2 teaspoon drop and shape into 30 clusters on wax paper let stand until firm about 30 minutes\n",
      "Entities: {'entities': [(2, 3, 'INGREDIENT'), (20, 25, 'QUANTITY'), (52, 59, 'QUANTITY'), (76, 88, 'QUANTITY'), (97, 104, 'QUANTITY'), (124, 136, 'QUANTITY'), (157, 164, 'QUANTITY'), (201, 202, 'INGREDIENT'), (209, 216, 'QUANTITY'), (429, 439, 'QUANTITY')]}\n",
      "--------------------------------------------------\n",
      "Text: jewell ball s chicken 1 small jar chipped beef cut up 4 boned chicken breasts 1 can cream of mushroom soup 1 carton sour cream place chipped beef on bottom of baking dish place chicken on top of beef mix soup and cream together pour over chicken bake uncovered at 275 for 3 hours\n",
      "Entities: {'entities': [(12, 13, 'INGREDIENT')]}\n",
      "--------------------------------------------------\n",
      "Text: creamy corn 2 16 ounce package frozen corn 1 8 ounce package cream cheese cubed 0.33 cup butter cubed 0.5 teaspoon garlic powder 0.5 teaspoon salt 0.25 teaspoon pepper in a slow cooker combine all ingredients cover and cook on low for 4 hours or until heated through and cheese is melted stir well before serving yields 6 servings\n",
      "Entities: {'entities': [(12, 22, 'QUANTITY'), (43, 52, 'QUANTITY'), (80, 88, 'QUANTITY'), (102, 114, 'QUANTITY'), (129, 141, 'QUANTITY'), (147, 160, 'QUANTITY'), (171, 172, 'INGREDIENT')]}\n",
      "--------------------------------------------------\n",
      "Text: reeses cup candy 1 cup peanut butter 0.75 cup graham cracker crumbs 1 cup melted butter 1 pound 3.5 cup powdered sugar 1 large package chocolate chips combine first four ingredients and press in 13 x 9 inch ungreased pan melt chocolate chips and spread over mixture refrigerate for about 20 minutes and cut into pieces before chocolate gets hard keep in refrigerator\n",
      "Entities: {'entities': [(17, 22, 'QUANTITY'), (37, 45, 'QUANTITY'), (68, 73, 'QUANTITY'), (88, 95, 'QUANTITY'), (96, 103, 'QUANTITY'), (198, 199, 'INGREDIENT')]}\n",
      "--------------------------------------------------\n",
      "Text: cheeseburger potato soup 6 baking potatoes 1 pound of extra lean ground beef 0.67 cup butter or margarine 6 cup milk 0.75 teaspoon salt 0.5 teaspoon pepper 1.5 cup 6 ounce shredded cheddar cheese divided 12 sliced bacon cooked crumbled and divided 4 green onion chopped and divided 1 8 ounce carton sour cream optional wash potatoes prick several times with a fork microwave them with a wet paper towel covering the potatoes on high for 6 8 minutes the potatoes should be soft ready to eat let them cool enough to handle cut in half lengthwise scoop out pulp and reserve discard shells brown ground beef until done drain any grease from the meat set aside when done meat will be added later melt butter in a large kettle over low heat add flour stirring until smooth cook 1 minute stirring constantly gradually add milk cook over medium heat stirring constantly until thickened and bubbly stir in potato ground beef salt pepper 1 cup of cheese 2 tablespoon of green onion and 0.5 cup of bacon cook until heated do not boil stir in sour cream if desired cook until heated do not boil sprinkle with remaining cheese bacon and green onions\n",
      "Entities: {'entities': [(43, 50, 'QUANTITY'), (77, 85, 'QUANTITY'), (106, 111, 'QUANTITY'), (117, 130, 'QUANTITY'), (136, 148, 'QUANTITY'), (156, 163, 'QUANTITY'), (164, 171, 'QUANTITY'), (282, 291, 'QUANTITY'), (358, 359, 'INGREDIENT'), (385, 386, 'INGREDIENT'), (706, 707, 'INGREDIENT'), (928, 933, 'QUANTITY'), (944, 956, 'QUANTITY'), (976, 983, 'QUANTITY')]}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Viewing training data\n",
    "\n",
    "# Print the first 3 records of train_data\n",
    "for i in range(5):  # Adjust the range to display more or fewer records\n",
    "    print(f\"Text: {train_data[i][0]}\")\n",
    "    print(f\"Entities: {train_data[i][1]}\")\n",
    "    print(\"-\" * 50)  # Separator for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bf6d5de9-268e-4739-906f-087b6dbd3ca0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E870] Could not serialize the DocBin because it is too large. Consider splitting up your documents into several doc bins and serializing each separately. spacy.Corpus.v1 will search recursively for all *.spacy files if you provide a directory instead of a filename as the 'path'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages/spacy/tokens/_serialize.py:262\u001b[0m, in \u001b[0;36mDocBin.to_disk\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     file_\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n",
      "File \u001b[0;32m/courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages/spacy/tokens/_serialize.py:217\u001b[0m, in \u001b[0;36mDocBin.to_bytes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m     msg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_data\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_data\n\u001b[0;32m--> 217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m zlib\u001b[38;5;241m.\u001b[39mcompress(\u001b[43msrsly\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmsgpack_dumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages/srsly/_msgpack_api.py:14\u001b[0m, in \u001b[0;36mmsgpack_dumps\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Serialize an object to a msgpack byte string.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mdata: The data to serialize.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mRETURNS (bytes): The serialized bytes.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmsgpack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_bin_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages/srsly/msgpack/__init__.py:60\u001b[0m, in \u001b[0;36mpackb\u001b[0;34m(o, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03mPack an object and return the packed bytes.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPacker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages/srsly/msgpack/_packer.pyx:297\u001b[0m, in \u001b[0;36msrsly.msgpack._packer.Packer.pack\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages/srsly/msgpack/_packer.pyx:294\u001b[0m, in \u001b[0;36msrsly.msgpack._packer.Packer.pack\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages/srsly/msgpack/_packer.pyx:283\u001b[0m, in \u001b[0;36msrsly.msgpack._packer.Packer._pack\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages/srsly/msgpack/_packer.pyx:231\u001b[0m, in \u001b[0;36msrsly.msgpack._packer.Packer._pack_inner\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages/srsly/msgpack/_packer.pyx:283\u001b[0m, in \u001b[0;36msrsly.msgpack._packer.Packer._pack\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages/srsly/msgpack/_packer.pyx:207\u001b[0m, in \u001b[0;36msrsly.msgpack._packer.Packer._pack_inner\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: bytes object is too large",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m         doc_bin\u001b[38;5;241m.\u001b[39madd(doc)\n\u001b[1;32m     13\u001b[0m     doc_bin\u001b[38;5;241m.\u001b[39mto_disk(output_file)\n\u001b[0;32m---> 15\u001b[0m \u001b[43msave_training_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining_data.spacy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 13\u001b[0m, in \u001b[0;36msave_training_data\u001b[0;34m(data, output_file)\u001b[0m\n\u001b[1;32m     11\u001b[0m     doc\u001b[38;5;241m.\u001b[39ments \u001b[38;5;241m=\u001b[39m ents\n\u001b[1;32m     12\u001b[0m     doc_bin\u001b[38;5;241m.\u001b[39madd(doc)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mdoc_bin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_disk\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages/spacy/tokens/_serialize.py:264\u001b[0m, in \u001b[0;36mDocBin.to_disk\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    262\u001b[0m     file_\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_bytes())\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE870)\n",
      "\u001b[0;31mValueError\u001b[0m: [E870] Could not serialize the DocBin because it is too large. Consider splitting up your documents into several doc bins and serializing each separately. spacy.Corpus.v1 will search recursively for all *.spacy files if you provide a directory instead of a filename as the 'path'."
     ]
    }
   ],
   "source": [
    "def save_training_data(data, output_file):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    doc_bin = DocBin()\n",
    "    for text, annotations in data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        ents = []\n",
    "        for start, end, label in annotations[\"entities\"]:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span is not None:\n",
    "                ents.append(span)\n",
    "        doc.ents = ents\n",
    "        doc_bin.add(doc)\n",
    "    doc_bin.to_disk(output_file)\n",
    "\n",
    "save_training_data(train_data, 'training_data.spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55775dc5-2f0f-47d7-b9e4-4b53ff64701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save in batches\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "def save_training_data(data, output_dir, chunk_size=5000):\n",
    "    \"\"\"Splits data into smaller DocBins and saves each as a separate file.\"\"\"\n",
    "    nlp = spacy.blank(\"en\")\n",
    "\n",
    "    for i in range(0, len(data), chunk_size):\n",
    "        doc_bin = DocBin()\n",
    "        batch = data[i:i + chunk_size]  # Get a chunk of data\n",
    "\n",
    "        for text, annotations in batch:\n",
    "            doc = nlp.make_doc(text)\n",
    "            ents = []\n",
    "            for start, end, label in annotations[\"entities\"]:\n",
    "                span = doc.char_span(start, end, label=label)\n",
    "                if span is not None:\n",
    "                    ents.append(span)\n",
    "            doc.ents = ents\n",
    "            doc_bin.add(doc)\n",
    "\n",
    "        output_file = f\"{output_dir}/training_data_part_{i // chunk_size}.spacy\"\n",
    "        doc_bin.to_disk(output_file)\n",
    "        print(f\" Saved {output_file} with {len(batch)} samples.\")\n",
    "\n",
    "# Save training data in smaller chunks\n",
    "save_training_data(train_data, 'training_data', chunk_size=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f193f65-fe70-45fd-9a33-60b53a25df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training data\n",
    "\n",
    "import glob\n",
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "def load_training_data(directory):\n",
    "    \"\"\"Loads multiple DocBin files and merges them into a single DocBin.\"\"\"\n",
    "    doc_bin = DocBin()  # Create an empty DocBin\n",
    "\n",
    "    # Load all .spacy files from the directory\n",
    "    for file in glob.glob(f\"{directory}/*.spacy\"):\n",
    "        print(f\"ðŸ“‚ Loading {file} ...\")\n",
    "        temp_bin = DocBin().from_disk(file)  # Load individual file\n",
    "        for doc in temp_bin.get_docs(nlp.vocab):  # Add docs to main DocBin\n",
    "            doc_bin.add(doc)\n",
    "\n",
    "    return doc_bin\n",
    "\n",
    "# Load all training data\n",
    "nlp = spacy.blank(\"en\")  # Initialize NLP pipeline\n",
    "db = load_training_data(\"training_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0661f11-18ff-47d5-a7a8-04d3aee143af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from spacy.training import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "# Load the training data\n",
    "#nlp = spacy.blank(\"en\")\n",
    "#db = DocBin().from_disk(\"training_data.spacy\")\n",
    "docs = list(db.get_docs(nlp.vocab))\n",
    "\n",
    "# Create the NER component and add it to the pipeline\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\", last=True)\n",
    "\n",
    "# Add the labels to the NER component\n",
    "for _, annotations in train_data:\n",
    "    for ent in annotations.get('entities'):\n",
    "        ner.add_label(ent[2])\n",
    "\n",
    "# Disable other pipes during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(5):  # 5 iterations\n",
    "        random.shuffle(docs)\n",
    "        losses = {}\n",
    "        batches = minibatch(docs, size=compounding(4.0, 32.0, 1.5))\n",
    "        for batch in batches:\n",
    "            for doc in batch:\n",
    "                example = Example.from_dict(doc, {\"entities\": [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]})\n",
    "                nlp.update([example], drop=0.5, losses=losses)\n",
    "        print(f\"Iteration {itn}, Losses: {losses}\")\n",
    "\n",
    "# Save the trained model to disk\n",
    "nlp.to_disk(\"ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e87f2b8-bba3-4406-b62f-c1bb673d8dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the custom NER\n",
    "doc = nlp(\"Add 2 cups of flour and 1 tablespoon of sugar.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3398ea24-3450-4c8c-b7fc-23b441d0a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c64f54a-21d4-407b-80b7-cda04106c786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data):\n",
    "    examples = [Example.from_dict(nlp.make_doc(text), annotations) for text, annotations in test_data]\n",
    "    scorer = model.evaluate(examples)\n",
    "    return scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b96aba-9b25-43ae-8580-862d8aef0d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = evaluate_model(nlp, train_data)\n",
    "print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e62489-d05f-4974-9c81-25d2ce444574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on some examples\n",
    "test_texts = \"In a heavy 2-quart saucepan, mix 2 lbs brown sugar, nuts, evaporated milk and butter or margarine. Stir over medium heat until mixture bubbles all over top. Boil and stir 5 minutes more. Take off heat. Stir in vanilla and cereal; mix well. Using 2 teaspoons, drop and shape into 30 clusters on wax paper.Let stand until firm, about 30 minutes.\"\n",
    "doc = nlp(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11649d21-1f5e-4056-8e1e-79d74e44bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cee483-4baa-4d3d-8c13-4d74ba7ef2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_text = \"\"\"Peel potatoes and keep aside\n",
    "Heat oil in a wok\n",
    "Crackle mustard seeds and urad dal\n",
    "Add Kari Patta\n",
    "soute ginger garlic paste and chopped onion\n",
    "Aad potatoes and soute for 1 to 2minutes\n",
    "Add all dry masala\n",
    "Cook with lid on slow flame for 10 minutes\n",
    "Garnish with chopped coriander\n",
    "Serve hot with roti or poori\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cdb19a-5177-4c36-8cb4-2cfec7759a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(real_text)\n",
    "\n",
    "displacy.render(doc2, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df7d52-2e94-4e45-82e6-c192a7530346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "entities = []\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"QUANTITY\":\n",
    "        for i_ent in doc.ents:\n",
    "            if i_ent.label_ == \"INGREDIENT\" and i_ent.start == ent.end:\n",
    "                entities.append({\"ingredient\": i_ent.text, \"quantity\": ent.text})\n",
    "                break\n",
    "    elif ent.label_ == \"INGREDIENT\":\n",
    "        if not any(e[\"ingredient\"] == ent.text for e in entities):\n",
    "            entities.append({\"ingredient\": ent.text, \"quantity\": \"\"})\n",
    "\n",
    "entities_json = json.dumps(entities, indent=2)\n",
    "\n",
    "print(entities_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e42aee3-3b40-4555-b120-333953853128",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
