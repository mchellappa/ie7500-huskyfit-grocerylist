{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8068ac8-5091-4b88-a9fe-7b23b3efb33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "455fe49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1784913\n",
      "Testing set size: 446229\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"recipe_dataset_large.csv\")  # Replace with your actual file path\n",
    "\n",
    "# Split the dataset into 80% training and 20% testing\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Testing set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dcc561b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy training data saved in chunks.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import re\n",
    "import ast\n",
    "\n",
    "def extract_quantities(text):\n",
    "    \"\"\"Extract quantities from the text.\"\"\"\n",
    "    quantity_patterns = [\n",
    "        r'\\b\\d+\\s*(?:cup|cups|tbsp|tablespoon|tablespoons|tsp|teaspoon|teaspoons|oz|ounce|ounces|lb|pound|pounds|g|gram|grams|kg|kilogram|kilograms|ml|milliliter|milliliters|l|liter|liters)\\b',\n",
    "        r'\\b\\d+/\\d+\\s*(?:cup|cups|tbsp|tablespoon|tablespoons|tsp|teaspoon|teaspoons|oz|ounce|ounces|lb|pound|pounds|g|gram|grams|kg|kilogram|kilograms|ml|milliliter|milliliters|l|liter|liters)\\b',\n",
    "        r'\\b\\d+\\s*(?:and\\s*\\d+/\\d+)?\\s*(?:cup|cups|tbsp|tablespoon|tablespoons|tsp|teaspoon|teaspoons|oz|ounce|ounces|lb|pound|pounds|g|gram|grams|kg|kilogram|kilograms|ml|milliliter|milliliters|l|liter|liters)\\b',\n",
    "        r'\\b\\d+\\s*(?:-\\s*\\d+)?\\s*(?:cup|cups|tbsp|tablespoon|tablespoons|tsp|teaspoon|teaspoons|oz|ounce|ounces|lb|pound|pounds|g|gram|grams|kg|kilogram|kilograms|ml|milliliter|milliliters|l|liter|liters)\\b',\n",
    "        r'\\b\\d+\\s*(?:to\\s*\\d+)?\\s*(?:cup|cups|tbsp|tablespoon|tablespoons|tsp|teaspoon|teaspoons|oz|ounce|ounces|lb|pound|pounds|g|gram|grams|kg|kilogram|kilograms|ml|milliliter|milliliters|l|liter|liters)\\b',\n",
    "        r'\\b\\d+\\s*(?:\\.\\d+)?\\s*(?:cup|cups|tbsp|tablespoon|tablespoons|tsp|teaspoon|teaspoons|oz|ounce|ounces|lb|pound|pounds|g|gram|grams|kg|kilogram|kilograms|ml|milliliter|milliliters|l|liter|liters)\\b',\n",
    "        r'\\bÂ½\\s*(?:cup|cups|tbsp|tablespoon|tablespoons|tsp|teaspoon|teaspoons|oz|ounce|ounces|lb|pound|pounds|g|gram|grams|kg|kilogram|kilograms|ml|milliliter|milliliters|l|liter|liters)\\b'\n",
    "    ]\n",
    "    entities = []\n",
    "    for pattern in quantity_patterns:\n",
    "        matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            entities.append((match.start(), match.end(), \"QUANTITY\"))\n",
    "    return entities\n",
    "\n",
    "def get_clean_ingredients(ingredient_list):\n",
    "    \"\"\"Clean the ingredients list to be used for NER.\"\"\"\n",
    "    try:\n",
    "        return ast.literal_eval(ingredient_list)  # Convert string to list\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "def extract_ingredients(text, ingredients):\n",
    "    \"\"\"Extract ingredients from the text.\"\"\"\n",
    "    entities = []\n",
    "    for ingredient in ingredients:\n",
    "        start_idx = text.lower().find(ingredient.lower())\n",
    "        if start_idx != -1:\n",
    "            end_idx = start_idx + len(ingredient)\n",
    "            entities.append((start_idx, end_idx, \"INGREDIENT\"))\n",
    "    return entities\n",
    "\n",
    "# Initialize SpaCy blank English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 1000  # Adjust based on your memory capacity\n",
    "\n",
    "# Process the training data in chunks\n",
    "for i in range(0, len(train_df), chunk_size):\n",
    "    doc_bin = DocBin()\n",
    "    chunk = train_df.iloc[i:i + chunk_size]\n",
    "    \n",
    "    for _, row in chunk.iterrows():\n",
    "        recipe_text = row['ingredients']  # Assuming 'ingredients' column has the list of ingredients\n",
    "        ner_labels = get_clean_ingredients(row['NER'])  # Clean the NER column\n",
    "\n",
    "        # Extract both quantities and ingredients from the text\n",
    "        quantity_entities = extract_quantities(recipe_text)\n",
    "        ingredient_entities = extract_ingredients(recipe_text, ner_labels)\n",
    "\n",
    "        # Combine quantity and ingredient entities\n",
    "        all_entities = quantity_entities + ingredient_entities\n",
    "\n",
    "        # Make sure there are no overlapping entities\n",
    "        added_spans = []\n",
    "        entities_to_add = []\n",
    "        doc = nlp.make_doc(recipe_text)\n",
    "\n",
    "        for start, end, label in all_entities:\n",
    "            # Check for overlapping spans\n",
    "            if all(not (start < existing_end and end > existing_start) for existing_start, existing_end in added_spans):\n",
    "                span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "                if span:\n",
    "                    entities_to_add.append(span)\n",
    "                    added_spans.append((start, end))\n",
    "\n",
    "        # Assign entities to the doc and add to DocBin\n",
    "        if entities_to_add:\n",
    "            doc.ents = entities_to_add\n",
    "            doc_bin.add(doc)\n",
    "\n",
    "    # Save the processed training data in SpaCy format for each chunk\n",
    "    doc_bin.to_disk(f\"chunks/ner_training_data_chunk_{i // chunk_size}.spacy\")\n",
    "\n",
    "print(\"SpaCy training data saved in chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c1bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.training import Example\n",
    "import glob\n",
    "\n",
    "# Load the training data from chunks\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin()\n",
    "\n",
    "for chunk_file in glob.glob(\"chunks/ner_training_data_chunk_*.spacy\"):\n",
    "    chunk_bin = DocBin().from_disk(chunk_file)\n",
    "    for doc in chunk_bin.get_docs(nlp.vocab):\n",
    "        doc_bin.add(doc)\n",
    "\n",
    "docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "\n",
    "# Create a new entity recognizer\n",
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.add_pipe(\"ner\")\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "# Add labels to the entity recognizer\n",
    "for doc in docs:\n",
    "    for ent in doc.ents:\n",
    "        ner.add_label(ent.label_)\n",
    "\n",
    "# Disable other pipelines during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for i in range(20):  # Number of iterations\n",
    "        for doc in docs:\n",
    "            example = Example.from_dict(doc, {\"entities\": [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]})\n",
    "            nlp.update([example], drop=0.5, sgd=optimizer)\n",
    "\n",
    "# Save the model\n",
    "nlp.to_disk(\"./output/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.training import Example\n",
    "\n",
    "# Load the trained model\n",
    "nlp = spacy.load(\"./output/model-best\")\n",
    "\n",
    "# Prepare the test data\n",
    "test_docs = []\n",
    "for _, row in test_df.iterrows():\n",
    "    recipe_text = row['ingredients']  # Assuming 'ingredients' column has the list of ingredients\n",
    "    ner_labels = get_clean_ingredients(row['NER'])  # Clean the NER column\n",
    "\n",
    "    # Extract both quantities and ingredients from the text\n",
    "    quantity_entities = extract_quantities(recipe_text)\n",
    "    ingredient_entities = extract_ingredients(recipe_text, ner_labels)\n",
    "\n",
    "    # Combine quantity and ingredient entities\n",
    "    all_entities = quantity_entities + ingredient_entities\n",
    "\n",
    "    # Create a SpaCy Example object\n",
    "    doc = nlp.make_doc(recipe_text)\n",
    "    example = Example.from_dict(doc, {\"entities\": [(start, end, label) for start, end, label in all_entities]})\n",
    "    test_docs.append(example)\n",
    "\n",
    "# Evaluate the model\n",
    "scorer = Scorer()\n",
    "for example in test_docs:\n",
    "    nlp.update([example], drop=0.0)\n",
    "    scorer.score(example)\n",
    "\n",
    "# Print evaluation results\n",
    "print(scorer.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b426459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the custom NER\n",
    "doc = nlp(\"Add 2 cups of flour and 1 tablespoon of sugar.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
