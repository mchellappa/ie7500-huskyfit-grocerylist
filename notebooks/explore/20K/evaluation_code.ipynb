{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(test_data, actual_data, nlp):\n",
    "    predicted_data = [(text, {\"entities\": []}) for text in test_data]\n",
    "    \n",
    "    # Generate predictions for all texts\n",
    "    for j, (text, annotations) in enumerate(predicted_data):\n",
    "        doc = nlp(text)\n",
    "        entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        predicted_data[j] = (text, {\"entities\": entities})\n",
    "\n",
    "    examples = []\n",
    "    for actual, predicted in zip(actual_data, predicted_data):\n",
    "        text = actual[0]\n",
    "        actual_anns = actual[1]\n",
    "        predicted_anns = predicted[1]\n",
    "\n",
    "        doc_actual = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc_actual, actual_anns)\n",
    "\n",
    "        doc_predicted = nlp.make_doc(text)\n",
    "        predicted_spans = []\n",
    "        for start, end, label in predicted_anns[\"entities\"]:\n",
    "            span = doc_predicted.char_span(start, end, label=label)\n",
    "            if span is not None:\n",
    "                predicted_spans.append(span)\n",
    "        doc_predicted.ents = predicted_spans\n",
    "        example.predicted = doc_predicted\n",
    "        examples.append(example)\n",
    "\n",
    "    scorer = Scorer()\n",
    "    metrics = scorer.score(examples)\n",
    "    \n",
    "    return metrics\n",
    "    \n",
    "def print_metrics(metrics):\n",
    "    print(\"Model Metrics:\")\n",
    "    print(f\"Token Accuracy: {metrics['token_acc']}\")\n",
    "    print(f\"Token Precision: {metrics['token_p']}\")\n",
    "    print(f\"Token Recall: {metrics['token_r']}\")\n",
    "    print(f\"Token F1 Score: {metrics['token_f']}\")\n",
    "    print(f\"Entity Precision: {metrics['ents_p']}\")\n",
    "    print(f\"Entity Recall: {metrics['ents_r']}\")\n",
    "    print(f\"Entity F1 Score: {metrics['ents_f']}\")\n",
    "\n",
    "    # Prepare data for entity metrics in a table format\n",
    "    entity_data = []\n",
    "    for entity, scores in metrics['ents_per_type'].items():\n",
    "        entity_data.append([entity, scores['p'], scores['r'], scores['f']])\n",
    "    \n",
    "    # Print the table\n",
    "    print(tabulate(entity_data, headers=[\"Entity\", \"Precision\", \"Recall\", \"F1 Score\"], tablefmt=\"grid\"))\n",
    "    print(\"\\n\")\n",
    "\n",
    "def save_metrics_and_plot(parameter_sets, all_metrics):\n",
    "    # Checking if the file exists and if it has been written before\n",
    "    file_exists = os.path.isfile(\"metrics_summary.csv\")\n",
    "    \n",
    "    # Open the file and append metrics\n",
    "    with open(\"metrics_summary.csv\", \"a\") as f:\n",
    "        # Only write the header if the file doesn't exist or is empty\n",
    "        if not file_exists:\n",
    "            f.write(\"model,parameter,precision,recall,f1_score\\n\")\n",
    "\n",
    "        # Write the metrics for each parameter set\n",
    "        for i, metric in enumerate(all_metrics):\n",
    "            f.write(f\"model_{i + 1},{parameter_sets[i]},{metric['ents_p']},{metric['ents_r']},{metric['ents_f']}\\n\")\n",
    "\n",
    "    # Collect metrics for visualization\n",
    "    precisions = [metric[\"ents_p\"] for metrics in all_metrics]\n",
    "    recalls = [metric[\"ents_r\"] for metrics in all_metrics]\n",
    "    f1_scores = [metric[\"ents_f\"] for metrics in all_metrics]\n",
    "    \n",
    "    return precisions, recalls, f1_scores\n",
    "\n",
    "parameter_sets = [\n",
    "    {\"max_iterations\": 40, \"initial_lr\": 1e-3, \"dropout_rate\": 0.5, \"min_loss_improvement\": 7000, \"patience\": 6, \"decay_interval\":5},\n",
    "    {\"max_iterations\": 35, \"initial_lr\": 5e-3, \"dropout_rate\": 0.5, \"min_loss_improvement\": 7500, \"patience\": 6, \"decay_interval\":5},\n",
    "    {\"max_iterations\": 35, \"initial_lr\": 1e-2, \"dropout_rate\": 0.5, \"min_loss_improvement\": 5000, \"patience\": 6, \"decay_interval\":5},\n",
    "    {\"max_iterations\": 35, \"initial_lr\": 1e-2, \"dropout_rate\": 0.6, \"min_loss_improvement\": 6000, \"patience\": 5, \"decay_interval\":4}\n",
    "]\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "all_metrics = []\n",
    "\n",
    "for i in range(1, 5):\n",
    "        print(f\"Evaluating model: ner_model_{i}\")\n",
    "        model_path = f\"ner_model_{i}\"\n",
    "        nlp = spacy.load(model_path)\n",
    "\n",
    "        metrics = evaluate_model(test_df['test_recipe'].tolist(), actual_data, nlp)\n",
    "\n",
    "        #Call the custom metrics printing function\n",
    "        print_metrics(metrics)\n",
    "        all_metrics.append(metrics)\n",
    "    \n",
    "        precision, recall, f1_score = save_metrics_and_plot(parameter_sets, all_metrics)\n",
    "        \n",
    "        # Append metrics to lists for plotting\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f1_score)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
